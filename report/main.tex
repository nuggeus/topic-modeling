\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{enumerate}

\title{Topic Modeling Research}
\author{Melody Jiang}
\date{May 2019}

\begin{document}

\maketitle

\section{Possible Research Directions}

\subsection{Two main approaches to clustering}

\begin{enumerate}[(i)]
  \item \textbf{Distance-based} clustering. Using this approach, we analyze matrix of pairwise distances. Namely, suppose $y_i$ and $y_j$ are data points, then $D$ is a distance matrix whose $d_{ij}$ entry represents distance between $y_i$ and $y_j$.
  \item \textbf{Model-based} clustering. For example, $y_i \sim \sum_{h = 1}^k \pi_h k(\theta_h)$.
\end{enumerate}

\subsection{Two main problems in clustering}

\begin{enumerate}[(i)]
  \item sensitivity to kernel
  \item issues in high dimensions (large p)
\end{enumerate}

\subsection{Semi-solutions}

\begin{enumerate}
  \item \textbf{C-Bayes}. All derivations from assumed models (e.g. kernel misspecification). See \cite{miller2018robust}.
  \item \textbf{Model plus distance-based clustering}.
  See \cite{duan2018bayesian}.
  \item \textbf{Calculating better distances}. E.g., geodesic or intrinsic distace (Didong Li \& Dunson, in preparation).
  \item \textbf{To address issues in high dimensions}, cluster on the latent variable level or varational autoencoder (VAE). 
\end{enumerate}




\bibliography{tm-bib}
\bibliographystyle{apalike}

\end{document}
